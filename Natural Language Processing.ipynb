{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, How are you doing Today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue.', 'You should not eat cardboard.']\n",
      "['Hello', 'Mr.', 'Smith', ',', 'How', 'are', 'you', 'doing', 'Today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "example_text = 'Hello Mr. Smith, How are you doing Today? The weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard.'\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))\n",
    "#for i in word_tokenize(example_text):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'How', 'Today', '?', 'The', 'weather', 'great', 'Python', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [w for w in word_tokenize(example_text) if not w in stop_words]\n",
    "print(filtered_words)\n",
    "#for i in word_tokenize(example_text):\n",
    "#    if i not in stop_words:\n",
    "#        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'veri', 'import', 'to', 'be', 'pythonli', 'while', 'you', 'are', 'python', 'in', 'python', '.', 'all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc']\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "example_words = ['Python','Pythoner','Pythoned','Pythoning','Pythonly']\n",
    "new_text = 'It is very important to be pythonly while you are pythoning in python. All pythoners have pythoned poorly at least once'\n",
    "ps = PorterStemmer()\n",
    "Is = ISRIStemmer()\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "new_word = word_tokenize(new_text)\n",
    "stemmed=[]\n",
    "\n",
    "for i in new_word:\n",
    "    stemmed.append(ps.stem(i))\n",
    "print(stemmed)\n",
    "\n",
    "#for i in example_words:\n",
    "    #print(ps.stem(i))\n",
    "    #print(Is.stem(i))\n",
    "    #print(ls.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parts of speech tagging\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "#process_content()                       \"remove co\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = pos_tag(words)\n",
    "            chunkgram = \"\"\"C:{<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkparser = nltk.RegexpParser(chunkgram)\n",
    "            chunked = chunkparser.parse(tagged)\n",
    "            print(chunked)                                #use chunked.draw() instead of print()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chinking\n",
    "#removal\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = pos_tag(words)\n",
    "            chunkgram = \"\"\"C:{<.*>+}\n",
    "                            }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkparser = nltk.RegexpParser(chunkgram)\n",
    "            chunked = chunkparser.parse(tagged)\n",
    "            print(chunked)                                     #use chunked.draw() instead of print()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Relationship\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged)\n",
    "            print(namedEnt)                     #use draw() instead of print()                           \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "#process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "best\n",
      "python\n",
      "cat\n",
      "run\n",
      "cactus\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('better',pos='a'))       #adjective\n",
    "print(lemmatizer.lemmatize('best','a'))\n",
    "print(lemmatizer.lemmatize('python'))\n",
    "print(lemmatizer.lemmatize('cats'))\n",
    "print(lemmatizer.lemmatize('run'))\n",
    "print(lemmatizer.lemmatize('cacti'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n",
      "Synonyms: {'secure', 'sound', 'near', 'just', 'proficient', 'honorable', 'serious', 'estimable', 'right', 'goodness', 'well', 'unspoilt', 'safe', 'practiced', 'dear', 'in_force', 'salutary', 'effective', 'upright', 'beneficial', 'trade_good', 'expert', 'adept', 'in_effect', 'thoroughly', 'unspoiled', 'commodity', 'undecomposed', 'respectable', 'skillful', 'good', 'dependable', 'soundly', 'ripe', 'full', 'honest', 'skilful'}\n",
      "Antonyms: {'badness', 'evilness', 'bad', 'evil', 'ill'}\n"
     ]
    }
   ],
   "source": [
    "#wordnet\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "#synonyms set\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns)\n",
    "#just a word\n",
    "print(syns[0].lemmas()[0].name())\n",
    "#definition\n",
    "print(syns[0].definition())\n",
    "#examples\n",
    "print(syns[0].examples())\n",
    "\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(\"Synonyms:\",set(synonyms))\n",
    "print(\"Antonyms:\",set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "0.2962962962962963\n",
      "0.32\n"
     ]
    }
   ],
   "source": [
    "#similarity check\n",
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "\n",
    "w3 = wordnet.synset('ship.n.01')\n",
    "w4 = wordnet.synset('sheep.n.01')\n",
    "\n",
    "w5 = wordnet.synset('cat.n.01')\n",
    "w6 = wordnet.synset('boat.n.01')\n",
    "\n",
    "\n",
    "print(w1.wup_similarity(w2))\n",
    "print(w3.wup_similarity(w4))\n",
    "print(w5.wup_similarity(w6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
